{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# KNN from Scratch"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using the KNN algorithm from scikit-learn is quite simple. When you use it, you don't even need to know how KNN works and classifies the data points. The goal should be to really understand what you are doing. This is the only way to make sure you understand when something doesn't go the way you expect or want it to. Many people remember things best when they implement them themselves. So, let's build our own K-Nearest-Neighbors classifier!\n",
    "\n",
    "**The purpose of this notebook is to help you remember the steps necessary to classify samples with KNN.**\n",
    "\n",
    "To test if your code works, you can use the Iris dataset as a data example.\n",
    "Let's make a plan and break this big task into smaller steps!\n",
    "\n",
    "\n",
    "1. What information and data does the algorithm need to train and predict the classes of new instances?\n",
    "This will be the input for our function! \n",
    "\n",
    "2. calculate the distance between the test point and each existing data point in the training data.\n",
    "3. determine the nearest k neighbors.\n",
    "4. make predictions based on these neighbors.\n",
    "\n",
    "You have already implemented a function to calculate the distance between points, which will now come in handy.\n",
    "\n",
    "A good way to get started, is to ignore the syntax and just write in simple text what you want your program to do aka **write pseudocode**. You can then start to build out some of the structure. What variables are you going to need? What kinds of logic? \n",
    "Knowing where you’re going can help you make fewer mistakes as you’re trying to get there.\n",
    "\n",
    "Note that for large data sets, the algorithm can take very long to classify because it has to calculate the distance between the test point and every other point in the data!\n",
    "\n",
    "You can check if your pseudo-code contains all necessary steps afterwards, when scrolling down to \"KNN algorithm from scratch\" where you find an example of a knn pseudocode."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import and Setup"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial import distance\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Load data\n",
    "df = pd.read_csv('data/iris.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Defining X and y \n",
    "y = df.species\n",
    "X = df.drop(['species'], axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Distance Metrics\n",
    "\n",
    "As already explained, KNN assigns a class to the test point based on the majority class of  K  nearest neighbors. In general, euclidean distance is used to find nearest neighbors, but other distance metrics can also be used.\n",
    "\n",
    "As the dimensionality of the feature space increases, the euclidean distance often becomes problematic due to the curse of dimensionality (discussed later).\n",
    "\n",
    "In such cases, alternative vector-based similarity measures (dot product, cosine similarity, etc) are used to find the nearest neighbors. This transforms the original metric space into one more amicable to point-to-point measurements.\n",
    "\n",
    "Another distance measure that you might consider is [Mahalanobis distance](https://en.wikipedia.org/wiki/Mahalanobis_distance). Mahalanobis distance attempts to weight features according to their probabilities. On some data sets that may be important.\n",
    "\n",
    "In general, it's probably a good idea to normalize the data at a minimum. Here's a link to the scikit-learn scaling package: http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html . You have to be a little circumspect about employing any technique where the answers change with scaling."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Implement distance function on your own\n",
    "def distance_self(pt1, pt2, c=2):\n",
    "    dist = np.power(sum([np.abs(xi-yi)**c for xi, yi in zip(pt1, pt2)]),1/c)\n",
    "    return dist"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Checking if distance function works as expected. Therefore the data has to be a numpy array not a pandas dataframe. \n",
    "X_train_array = np.array(X_train)\n",
    "X_test_array = np.array(X_test)\n",
    "\n",
    "# Calculate distance with self implemented function \n",
    "print('Self implemented:', distance_self(X_train_array[0], X_test_array[0]))\n",
    "\n",
    "# Calculating euclidean distance with numpy (as reference)\n",
    "print('Numpy:', np.linalg.norm(X_train_array[0] - X_test_array[0]))\n",
    "\n",
    "# Calculating euclidean distance with scipy (as reference)\n",
    "print('Scipy:', distance.euclidean(X_train_array[0], X_test_array[0]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## KNN ALgorithm from scratch\n",
    "\n",
    "\n",
    "Remember the steps:\n",
    "\n",
    "1. What information and data does the algorithm need to train and predict the classes of new instances?\n",
    "This will be the input for our function! \n",
    "\n",
    "2. calculate the distance between the test point and each existing data point in the training data.\n",
    "3. determine the nearest k neighbors.\n",
    "4. make predictions based on these neighbors.\n",
    "\n",
    "Hopefully you have already thought of your gameplan, also called pseudocode. If so, you can compare it to this one:\n",
    "```\n",
    "INPUT: X_train, y_train, X_test, k\n",
    "FOR each object_to_predict in X_test:\n",
    "    FOR each training_point, index in X_train:\n",
    "        calculate distance d between object_to_predict and training_point\n",
    "        store d and index\n",
    "    SORT distances d in increasing order\n",
    "    take first k items, get indices of those\n",
    "    calculate most common class of points at indices in y_train (prediction)\n",
    "    store prediction\n",
    "RETURN list of predictions\n",
    "````\n",
    "\n",
    "Time to code!\n",
    "Don't forget that it's good practice to document your own code! This way you can later understand what the purpose of each step was.\n",
    "Maybe you can even use your pseudo code as documentation :)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# KNN implementation\n",
    "\n",
    "def knn_self(X_train, X_test, y_train, k=5):\n",
    "    \"\"\"Implementation of an k-nearest neighbors classifier. \n",
    "    Returning predicted classes of samples to be predicted (X_test).\n",
    "\n",
    "    Args:\n",
    "        X_train (pd.DataFrame): Training data\n",
    "        y_train (pd.Series): Target values of training data\n",
    "        X_test (pd.DataFrame): Test data\n",
    "        k (int, optional): Number of nearest neighbors considered for the classification. Defaults to 5.\n",
    "    \"\"\"\n",
    "\n",
    "    pred_list = []                                            # Defining empty list to store the final predictions\n",
    "    \n",
    "    X_train = np.array(X_train)                               # Change format from dataframe to numpy array, since the self implemented... \n",
    "    X_test = np.array(X_test)                                 # ...distance function requires a numpy array as input format.\n",
    "    y_train = np.array(y_train)\n",
    "\n",
    "    for xp in X_test:                                         # Iterate through every test instance in order to predict a label \n",
    "        \n",
    "        dist_list = []                                        # Defining empty list to temporarily store the calculated distance for a test...\n",
    "                                                              # ...instance to all the single trainings instances. \n",
    "        for ind, xt in enumerate(X_train):                    # Iterate through every trainings instance to calculate the distance from the test instance. \n",
    "             \n",
    "            dist = distance_self(xp, xt)                      # Calculate the distance for a test instance to a training instances\n",
    "            dist_list.append((dist, ind))                     # Store the calculated distance and the index of the instance as tuple in list 'dist_list'\n",
    "                \n",
    "        list_sorted = sorted(dist_list)[:k]                   # Sort dist_list after distance and keep only the first k instances\n",
    "            \n",
    "        y_ind = [i[1] for i in list_sorted]                   # Get indexes from tuple of the k closest instances \n",
    "        y_labels = [y_train[i] for i in y_ind]                # Get labels from training labels for the collected indexes...\n",
    "                                                              # ...returns a list 'y_labels' with the labels for the k closest training instances.\n",
    "        y_pred = max(y_labels, key=y_labels.count)            # Get the most common label form the y_labels list --> Prediction for the test instance. \n",
    "            \n",
    "        pred_list.append(y_pred)                              # Store the prediction in the pred_list. And start the for loop again for new test instance.\n",
    "    \n",
    "    return pred_list                                          # Return a list with all the calculated predicitons for the test instances. "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Get predictions for X_test with the knn_self\n",
    "y_prediction = knn_self(X_train, X_test, y_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Evaluate the results of knn_self\n",
    "confusion_matrix(y_test, y_prediction)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Comparison with sklearn KNN implementation\n",
    "\n",
    "That will be interesting! Check out how your implementation performs in comparison to the one of sklearn!\n",
    "You can check the confusion matrix and the accuracy score of both algorithms.\n",
    "If you want, you can check which algorithm is faster!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Test vs. sklearn implementation\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)\n",
    "y_knn = knn.predict(X_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "confusion_matrix(y_test, y_knn)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.4 64-bit ('.venv': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "interpreter": {
   "hash": "a4807750d5fce81b58818f7ed66ec30aa7cd1fb02c0ca83dd500df7436beb8c8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}