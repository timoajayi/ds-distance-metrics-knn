{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN from Scratch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the KNN algorithm from scikit-learn is quite simple. When you use it, you don't even need to know how KNN works and classifies the data points. The goal should be to really understand what you are doing. This is the only way to make sure you understand when something doesn't go the way you expect or want it to. Many people remember things best when they implement them themselves. So, let's build our own K-Nearest-Neighbors classifier!\n",
    "\n",
    "**The purpose of this notebook is to help you remember the steps necessary to classify samples with KNN.**\n",
    "\n",
    "To test if your code works, you can use the Iris dataset as a data example.\n",
    "Let's make a plan and break this big task into smaller steps!\n",
    "\n",
    "\n",
    "1. What information and data does the algorithm need to train and predict the classes of new instances?\n",
    "This will be the input for our function! \n",
    "\n",
    "2. calculate the distance between the test point and each existing data point in the training data.\n",
    "3. determine the nearest k neighbors.\n",
    "4. make predictions based on these neighbors.\n",
    "\n",
    "You have already implemented a function to calculate the distance between points, which will now come in handy.\n",
    "\n",
    "A good way to get started, is to ignore the syntax and just write in simple text what you want your program to do aka **write pseudo-code**. You can then start to build out some of the structure. What variables are you going to need? What kinds of logic? \n",
    "Knowing where you’re going can help you make fewer mistakes as you’re trying to get there.\n",
    "\n",
    "Note that for large data sets, the algorithm can take very long to classify because it has to calculate the distance between the test point and every other point in the data!\n",
    "\n",
    "You can check if your pseudo-code contains all necessary steps afterwards, when scrolling down to \"KNN algorithm from scratch\" where you find an example of a knn pseudo-code."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial import distance\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('data/iris.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining X and y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance Metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As already explained, KNN assigns a class to the test point based on the majority class of  K  nearest neighbors. In general, euclidean distance is used to find nearest neighbors, but other distance metrics can also be used.\n",
    "\n",
    "As the dimensionality of the feature space increases, the euclidean distance often becomes problematic due to the curse of dimensionality (discussed later).\n",
    "\n",
    "In such cases, alternative vector-based similarity measures (dot product, cosine similarity, etc) are used to find the nearest neighbors. This transforms the original metric space into one more amicable to point-to-point measurements.\n",
    "\n",
    "Another distance measure that you might consider is [Mahalanobis distance](https://en.wikipedia.org/wiki/Mahalanobis_distance). Mahalanobis distance attempts to weight features according to their probabilities. On some data sets that may be important.\n",
    "\n",
    "In general, it's probably a good idea to normalize the data at a minimum. Here's a link to the scikit-learn scaling package: http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html . You have to be a little circumspect about employing any technique where the answers change with scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implemented own distance function \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Algorithm from scratch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Remember the steps:\n",
    "\n",
    "1. What information and data does the algorithm need to train and predict the classes of new instances?\n",
    "This will be the input for our function! \n",
    "\n",
    "2. calculate the distance between the test point and each existing data point in the training data.\n",
    "3. determine the nearest k neighbors.\n",
    "4. make predictions based on these neighbors.\n",
    "\n",
    "Hopefully you have already thought of your gameplan, also called pseudo-code. If so, you can compare it to this one:\n",
    "```\n",
    "INPUT: X_train, y_train, X_test, k\n",
    "FOR each object_to_predict in X_test:\n",
    "    FOR each training_point, index in X_train:\n",
    "        calculate distance d between object_to_predict and training_point\n",
    "        store d and index\n",
    "    SORT distances d in increasing order\n",
    "    take first k items, get indices of those\n",
    "    calculate most common class of points at indices in y_train (prediction)\n",
    "    store prediction\n",
    "RETURN list of predictions\n",
    "````\n",
    "\n",
    "Time to code!\n",
    "Don't forget that it's good practice to document your own code! This way you can later understand what the purpose of each step was.\n",
    "Maybe you can even use your pseudo code as documentation :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with sklearn knn implementation\n",
    "\n",
    "That will be interesting! Check out how your implementation performs in comparison to the one of sklearn!\n",
    "You can check the confusion matrix and the accuracy score of both algorithms.\n",
    "If you want, you can check which algorithm is faster!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a4807750d5fce81b58818f7ed66ec30aa7cd1fb02c0ca83dd500df7436beb8c8"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('.venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
